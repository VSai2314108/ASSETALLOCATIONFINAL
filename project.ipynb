{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Asset Allocation Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = \"SPY\"\n",
    "tickers = [\"DG\", \"MRK\", \"CB\", \"NOC\", \"BP\"] # chosen to be diverse across sectors\n",
    "\n",
    "# start date test\n",
    "test_start = \"2012-12-01\"\n",
    "test_end = \"2017-10-01\"\n",
    "\n",
    "# eval date test\n",
    "eval_start = \"2017-10-02\"\n",
    "eval_end = \"2019-10-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "\n",
    "query = \"https://query1.finance.yahoo.com/v7/finance/download/{}?period1=1325376000&period2=1577836800&interval=1mo&events=history&includeAdjustedClose=true\"\n",
    "for ticker in tickers+[market]:\n",
    "    os.makedirs(\"data/{}\".format(ticker), exist_ok=True)\n",
    "    urlretrieve(query.format(ticker), \"data/{}/{}.csv\".format(ticker, ticker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now compute the returns of our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute returns using the close  for each and add drop all other columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "for ticker in tickers+[market]:\n",
    "    df = pd.read_csv(\"data/{}/{}.csv\".format(ticker, ticker))\n",
    "    df[\"return\"] = df[\"Close\"].pct_change()\n",
    "    df = df.dropna()\n",
    "    df = df[[\"Date\", \"Close\", \"return\"]]\n",
    "    df.to_csv(\"data/{}/{}.csv\".format(ticker, ticker), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will collect our rate data and split it into monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the risk free rate and save to rates.csv\n",
    "rates_static = \"FRB_H15.csv\"\n",
    "rates_df = pd.read_csv(rates_static)\n",
    "\n",
    "# format the date form YYYY-MM to YYYY-MM-DD\n",
    "rates_df[\"Date\"] = rates_df[\"Date\"].apply(lambda x: x+\"-01\")\n",
    "rates_df[\"return\"] = rates_df[\"return\"]/1200\n",
    "\n",
    "# save it to rates.csv\n",
    "os.makedirs(\"data/rates\", exist_ok=True)\n",
    "rates_df.to_csv(\"data/rates/rates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will split our data into train and test periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "for ticker in tickers+[market, \"rates\"]:\n",
    "    # make dir in data\n",
    "    os.makedirs(\"data/{}\".format(ticker), exist_ok=True)\n",
    "    df = pd.read_csv(\"data/{}/{}.csv\".format(ticker, ticker))\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df = df.sort_values(\"Date\")\n",
    "    df_train = df[(df[\"Date\"] >= test_start) & (df[\"Date\"] <= test_end)]\n",
    "    df_test = df[(df[\"Date\"] >= eval_start) & (df[\"Date\"] <= eval_end)]\n",
    "    df_train.to_csv(\"data/{}/{}_train.csv\".format(ticker, ticker), index=False)\n",
    "    df_test.to_csv(\"data/{}/{}_test.csv\".format(ticker, ticker), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will aggregate our data into a training data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the combined training set\n",
    "train_data = pd.DataFrame()\n",
    "for ticker in tickers+[market]+[\"rates\"]:\n",
    "    df = pd.read_csv(\"data/{}/{}_train.csv\".format(ticker, ticker))\n",
    "    df = df[[\"Date\", \"return\"]]\n",
    "    df = df.rename(columns={\"return\": ticker})\n",
    "    if train_data.empty:\n",
    "        train_data = df\n",
    "    else:\n",
    "        train_data = pd.merge(train_data, df, on=\"Date\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can compute the basic statistics we need to devise multiple portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance Matrix\n",
      "                 DG       MRK            CB           NOC        BP  \\\n",
      "DG     4.508122e-03  0.000342  8.105542e-04  7.840403e-04  0.000359   \n",
      "MRK    3.424459e-04  0.002364  4.898439e-04  5.629656e-04  0.000636   \n",
      "CB     8.105542e-04  0.000490  1.456910e-03  6.276540e-04  0.000565   \n",
      "NOC    7.840403e-04  0.000563  6.276540e-04  1.848072e-03  0.000525   \n",
      "BP     3.588692e-04  0.000636  5.645264e-04  5.252179e-04  0.003952   \n",
      "SPY    6.894545e-04  0.000634  7.518302e-04  5.584653e-04  0.000739   \n",
      "rates  4.232179e-07 -0.000002  2.624179e-07 -4.793574e-07  0.000002   \n",
      "\n",
      "                SPY         rates  \n",
      "DG     6.894545e-04  4.232179e-07  \n",
      "MRK    6.339753e-04 -1.820762e-06  \n",
      "CB     7.518302e-04  2.624179e-07  \n",
      "NOC    5.584653e-04 -4.793574e-07  \n",
      "BP     7.392004e-04  2.135279e-06  \n",
      "SPY    7.845685e-04  3.310369e-07  \n",
      "rates  3.310369e-07  5.669386e-08  \n",
      "\n",
      "\n",
      "Correlation Matrix\n",
      "             DG       MRK        CB       NOC        BP       SPY     rates\n",
      "DG     1.000000  0.104901  0.316277  0.271632  0.085027  0.366600  0.026473\n",
      "MRK    0.104901  1.000000  0.263953  0.269344  0.208180  0.465524 -0.157279\n",
      "CB     0.316277  0.263953  1.000000  0.382511  0.235281  0.703215  0.028874\n",
      "NOC    0.271632  0.269344  0.382511  1.000000  0.194356  0.463790 -0.046831\n",
      "BP     0.085027  0.208180  0.235281  0.194356  1.000000  0.419822  0.142661\n",
      "SPY    0.366600  0.465524  0.703215  0.463790  0.419822  1.000000  0.049636\n",
      "rates  0.026473 -0.157279  0.028874 -0.046831  0.142661  0.049636  1.000000\n",
      "\n",
      "\n",
      "Deviation Matrix\n",
      "DG       0.067143\n",
      "MRK      0.048620\n",
      "CB       0.038169\n",
      "NOC      0.042989\n",
      "BP       0.062861\n",
      "SPY      0.028010\n",
      "rates    0.000238\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Expected Return\n",
      "DG       0.010427\n",
      "MRK      0.004874\n",
      "CB       0.011683\n",
      "NOC      0.026432\n",
      "BP       0.001467\n",
      "SPY      0.010480\n",
      "rates    0.000171\n",
      "dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna()\n",
    "train_data = train_data.set_index(\"Date\")\n",
    "train_data.to_csv(\"data/train_data.csv\")\n",
    "\n",
    "# cov\n",
    "cov_matrix = train_data.cov()\n",
    "cov_matrix.to_csv(\"data/cov_matrix.csv\")\n",
    "\n",
    "# corr\n",
    "corr_matrix = train_data.corr()\n",
    "corr_matrix.to_csv(\"data/corr_matrix.csv\")\n",
    "\n",
    "# deviation\n",
    "dev_matrix = train_data.std()\n",
    "dev_matrix.to_csv(\"data/dev_matrix.csv\")\n",
    "\n",
    "# expected return\n",
    "expected_return = train_data.mean()\n",
    "expected_return.to_csv(\"data/expected_return.csv\")\n",
    "\n",
    "print(\"Covariance Matrix\")\n",
    "print(cov_matrix)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Correlation Matrix\")\n",
    "print(corr_matrix)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Deviation Matrix\")\n",
    "print(dev_matrix)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Expected Return\")\n",
    "print(expected_return)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta DG: 0.8785577507728043\n",
      "Intercept DG: 0.0011989674873874815\n",
      "Residual Dev DG: 0.4757402186651434\n",
      "\n",
      "\n",
      "Beta MRK: 0.8106525543922\n",
      "Intercept MRK: -0.0036544905788099104\n",
      "Residual Dev MRK: 0.32778068959050455\n",
      "\n",
      "\n",
      "Beta CB: 0.9583274954573505\n",
      "Intercept CB: 0.0016325255467508955\n",
      "Residual Dev CB: 0.20667489290719473\n",
      "\n",
      "\n",
      "Beta NOC: 0.7126231420051568\n",
      "Intercept NOC: 0.01891431827260464\n",
      "Residual Dev NOC: 0.2900966104172669\n",
      "\n",
      "\n",
      "Beta BP: 0.9398282702149634\n",
      "Intercept BP: -0.008393210878596829\n",
      "Residual Dev BP: 0.43448845086189986\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute beta for each stock\n",
    "market_train = pd.read_csv(\"data/{}/{}_train.csv\".format(market, market))\n",
    "market_train = market_train.set_index(\"Date\")\n",
    "market_train = market_train.rename(columns={\"return\": market})\n",
    "market_train = market_train.dropna()\n",
    "\n",
    "# read in the rates test data\n",
    "rates_train = pd.read_csv(\"data/rates/rates_train.csv\")\n",
    "rates_train = rates_train.set_index(\"Date\")\n",
    "rates_train = rates_train.rename(columns={\"return\": \"rates\"})\n",
    "rates_train = rates_train.dropna()\n",
    "\n",
    "# compute the excess return for the market\n",
    "market_train[market] = market_train[market] - rates_train[\"rates\"]\n",
    "\n",
    "betas = pd.DataFrame()\n",
    "intercepts = pd.DataFrame()\n",
    "residual_dev = pd.DataFrame()\n",
    "for ticker in tickers:\n",
    "    df = pd.read_csv(\"data/{}/{}_train.csv\".format(ticker, ticker))\n",
    "    df = df.set_index(\"Date\")\n",
    "    df = df.rename(columns={\"return\": ticker})\n",
    "    df = df.dropna()\n",
    "    df[ticker] = df[ticker] - rates_train[\"rates\"]\n",
    "    df = pd.merge(df, market_train, on=\"Date\", how=\"inner\")\n",
    "    # beta = (df[ticker].cov(df[market]))/(df[market].var())\n",
    "    # intercept = df[ticker].mean() - beta*df[market].mean()\n",
    "    # betas.loc[ticker, \"beta\"] = beta\n",
    "    # intercepts.loc[ticker, \"intercept\"] = intercept\n",
    "    \n",
    "    # perform linear regression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    model = LinearRegression()\n",
    "    model.fit(df[[market]], df[[ticker]])\n",
    "    intercept = model.intercept_[0]\n",
    "    beta = model.coef_[0][0]\n",
    "    intercepts.loc[ticker, \"intercept\"] = intercept\n",
    "    betas.loc[ticker, \"beta\"] = beta\n",
    "    \n",
    "    # plot the regression line\n",
    "    import matplotlib.pyplot as plt\n",
    "    # plt.scatter(df[[market]], df[[ticker]])\n",
    "    # plt.plot(df[[market]], model.predict(df[[market]]), color=\"red\")\n",
    "    \n",
    "    # plot the residuals\n",
    "    # plt.scatter(df[[market]], df[[ticker]] - model.predict(df[[market]]))\n",
    "\n",
    "    residuals = df[[ticker]] - model.predict(df[[market]])\n",
    "    residuals = residuals**2\n",
    "    residual_dev.loc[ticker, \"residual_dev\"] = residuals.sum().values[0]**0.5\n",
    "    \n",
    "    print(\"Beta {ticker}: {beta}\".format(ticker=ticker, beta=beta))\n",
    "    print(\"Intercept {ticker}: {intercept}\".format(ticker=ticker, intercept=intercept))\n",
    "    print(\"Residual Dev {ticker}: {residual_dev}\".format(ticker=ticker, residual_dev=residual_dev.loc[ticker, \"residual_dev\"]))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "betas.to_csv(\"data/betas.csv\")\n",
    "intercepts.to_csv(\"data/intercepts.csv\")\n",
    "residual_dev.to_csv(\"data/residual_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Return DG: 0.011904761904761904\n",
      "Expected Market Return: 0.010309632619618819\n",
      "Expected Risk Free Rate: 0.0008166666666666\n",
      "Alpha DG: 0.27479764223067926\n",
      "\n",
      "\n",
      "Expected Return MRK: 0.01445978454136596\n",
      "Expected Market Return: 0.010309632619618819\n",
      "Expected Risk Free Rate: 0.0008166666666666\n",
      "Alpha MRK: 0.594762077618046\n",
      "\n",
      "\n",
      "Expected Return CB: 0.011966748880971942\n",
      "Expected Market Return: 0.010309632619618819\n",
      "Expected Risk Free Rate: 0.0008166666666666\n",
      "Alpha CB: 0.20527119281507408\n",
      "\n",
      "\n",
      "Expected Return NOC: 0.007151112395261482\n",
      "Expected Market Return: 0.010309632619618819\n",
      "Expected Risk Free Rate: 0.0008166666666666\n",
      "Alpha NOC: -0.04304614957459062\n",
      "\n",
      "\n",
      "Expected Return BP: 0.009515570934256057\n",
      "Expected Market Return: 0.010309632619618819\n",
      "Expected Risk Free Rate: 0.0008166666666666\n",
      "Alpha BP: -0.022285350318316903\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alphas\n",
    "# compute expected return using analyst data\n",
    "analyst = pd.read_csv(\"analysts.csv\")\n",
    "alphas = pd.DataFrame()\n",
    "# pull the price \n",
    "for ticker in tickers:\n",
    "    expected_price = analyst[analyst[\"Ticker\"] == ticker][\"Price\"].values[0]\n",
    "    dividend = analyst[analyst[\"Ticker\"] == ticker][\"Dividend\"].values[0]\n",
    "    train_data = pd.read_csv(\"data/{}/{}_train.csv\".format(ticker, ticker))\n",
    "    last_price = analyst[analyst[\"Ticker\"] == ticker][\"Current\"].values[0]\n",
    "    analyst_return = (expected_price + dividend - last_price) / last_price\n",
    "    \n",
    "    # compute the alpha\n",
    "    last_rf = rates_train[\"rates\"].iloc[-1]\n",
    "    last_rm = market_train[market].mean()\n",
    "    erm = analyst_return/12.0\n",
    "    alpha = (erm - last_rf - betas.loc[ticker, \"beta\"]*(last_rm - last_rf))*100\n",
    "    alphas.loc[ticker, \"alpha\"] = alpha\n",
    "    # print all intermediate values\n",
    "    print(\"Expected Return {ticker}: {erm}\".format(ticker=ticker, erm=erm))\n",
    "    print(\"Expected Market Return: {last_rm}\".format(last_rm=last_rm))\n",
    "    print(\"Expected Risk Free Rate: {last_rf}\".format(last_rf=last_rf))\n",
    "    print(\"Alpha {ticker}: {alpha}\".format(ticker=ticker, alpha=alpha))\n",
    "    print(\"\\n\")\n",
    "\n",
    "alphas.to_csv(\"data/alphas.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute residual variance for all stocks\n",
    "# residuals = pd.DataFrame()\n",
    "# for ticker in tickers:\n",
    "#     df = pd.read_csv(\"data/{}/{}_train.csv\".format(ticker, ticker))\n",
    "#     df = df.set_index(\"Date\")\n",
    "#     df = df.rename(columns={\"return\": ticker})\n",
    "#     df = df.dropna()\n",
    "#     df[ticker] = df[ticker] - rates_train[\"rates\"]\n",
    "#     df = pd.merge(df, market_train, on=\"Date\", how=\"inner\")\n",
    "#     df[\"residual\"] = df[ticker] - betas.loc[ticker, \"beta\"]*df[market] - intercepts.loc[ticker, \"intercept\"]\n",
    "#     residuals.loc[ticker, \"residual_variance\"] = df[\"residual\"].std()\n",
    "# residuals.to_csv(\"data/residuals.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Strategies\n",
    "### The assumption we are utilizing is that the CAL has a consistent slope hence we are only computing the risky portfolio.\n",
    "1. Equities equal allocation portfolio \n",
    "2. SP500 only\n",
    "3. optimal risky portfolio that matches SP500 expected return \n",
    "4. optimal risky portfolio that matches SP500 deviation\n",
    "5. Beta Scaled Portfolio (Growth Portfolio)\n",
    "6. Treynor-Black Allocation\n",
    "7. Markowitz allocation (market as an asset to simulate comparison to treynor black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = pd.DataFrame()\n",
    "# add the tickers + market to index\n",
    "final_weights.index = tickers + [market]\n",
    "\n",
    "# Algorithm 1 all equal weights\n",
    "weights = np.ones(len(tickers)+1)/(len(tickers)+1)\n",
    "final_weights[\"equal\"] = weights\n",
    "\n",
    "# Algorithm 2 only market\n",
    "weights = np.zeros(len(tickers)+1)\n",
    "weights[-1] = 1\n",
    "final_weights[\"market\"] = weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization data\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "train_data = pd.read_csv(\"data/train_data.csv\")\n",
    "\n",
    "# choose only the tickers and the market\n",
    "train_data = train_data[tickers+[market]]\n",
    "\n",
    "# compute the expected return\n",
    "expected_return = train_data.mean()\n",
    "\n",
    "# compute the covariance matrix\n",
    "cov_matrix = train_data.cov()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 3 minimum variance equal return to market\n",
    "targ_ret = expected_return.loc[market]\n",
    "\n",
    "def portfolio_dev(weights, cov_matrix):\n",
    "    return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "\n",
    "def constraint(weights, targ_ret, expected_return):\n",
    "    return np.dot(weights, expected_return) - targ_ret\n",
    "\n",
    "def constraint2(weights):\n",
    "    return np.sum(weights) - 1\n",
    "\n",
    "constraints = [{\"type\": \"eq\", \"fun\": constraint, \"args\": (targ_ret, expected_return)}, {\"type\": \"eq\", \"fun\": constraint2}]\n",
    "\n",
    "weights_guess = np.ones(len(tickers)+1)/(len(tickers)+1)\n",
    "\n",
    "optimal_portfolio = minimize(portfolio_dev, weights_guess, args=(cov_matrix), method='SLSQP', constraints=constraints)\n",
    "final_weights.loc[[ticker for ticker in tickers]+[market], \"min_var_mark_return\"] = optimal_portfolio.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 4 equal variance maximal return\n",
    "\n",
    "targ_var = dev_matrix.loc[market]\n",
    "\n",
    "def ret_func(weights, expected_return):\n",
    "    return -np.dot(weights, expected_return)\n",
    "\n",
    "def constraint(weights, targ_var, cov_matrix):\n",
    "    return portfolio_dev(weights, cov_matrix) - targ_var\n",
    "\n",
    "def constraint2(weights):\n",
    "    return np.sum(weights) - 1\n",
    "\n",
    "constraints = [{\"type\": \"eq\", \"fun\": constraint, \"args\": (targ_var, cov_matrix)}, {\"type\": \"eq\", \"fun\": constraint2}]\n",
    "\n",
    "optimal_portfolio = minimize(ret_func, weights_guess, args=(expected_return), method='SLSQP', constraints=constraints)\n",
    "final_weights.loc[[ticker for ticker in tickers]+[market], \"equal_var_max_return\"] = optimal_portfolio.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 5 Beta Scaled\n",
    "betas = pd.read_csv(\"data/betas.csv\")\n",
    "betas.set_index(\"Unnamed: 0\", inplace=True)\n",
    "betas.index.name = \"Ticker\"\n",
    "betas.loc[market, \"beta\"] = 1\n",
    "\n",
    "scaled_betas = betas[\"beta\"] / betas[\"beta\"].sum()\n",
    "\n",
    "final_weights.loc[[ticker for ticker in tickers]+[market], \"beta_scaled\"] = scaled_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm six - Markowitz\n",
    "\n",
    "# optimization function\n",
    "def portfolio_dev(weights, cov_matrix):\n",
    "    return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "\n",
    "# constraints - we are okay with short selling\n",
    "def constraint(weights):\n",
    "    return weights.sum() - 1\n",
    "constraints = {\"type\": \"eq\", \"fun\": constraint}\n",
    "\n",
    "# initial guess\n",
    "weights_guess = np.random.rand(len(tickers)+1)\n",
    "\n",
    "# minimize\n",
    "from typing import final\n",
    "from scipy.optimize import minimize\n",
    "optimal_portfolio = minimize(portfolio_dev, weights_guess, args=(cov_matrix), method='SLSQP', constraints=constraints)\n",
    "\n",
    "# save to final weights\n",
    "final_weights.loc[[ticker for ticker in tickers]+[market], \"markowitz\"] = optimal_portfolio.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Position:  0.8179381580415437\n",
      "Beta Portfolio:  0.8863449402080142\n",
      "Adjusted Initial Position:  0.7483677855865897\n",
      "Expected Return Portfolio:  0.32721092755967873\n",
      "Variance Portfolio:  0.02241204501304662\n",
      "Weights:          equal  market  min_var_mark_return  equal_var_max_return  beta_scaled  \\\n",
      "DG   0.166667     0.0             0.025749             -0.001143     0.165766   \n",
      "MRK  0.166667     0.0             0.100610              0.015739     0.152954   \n",
      "CB   0.166667     0.0             0.046923              0.027365     0.180817   \n",
      "NOC  0.166667     0.0             0.053161              0.295512     0.134457   \n",
      "BP   0.166667     0.0             0.037610             -0.023064     0.177326   \n",
      "SPY  0.166667     1.0             0.735948              0.685590     0.188680   \n",
      "\n",
      "     markowitz   treynor  \n",
      "DG    0.017442  0.083162  \n",
      "MRK   0.072848  0.379167  \n",
      "CB    0.036422  0.329159  \n",
      "NOC   0.133218 -0.035035  \n",
      "BP    0.015318 -0.008086  \n",
      "SPY   0.724751  0.251632  \n"
     ]
    }
   ],
   "source": [
    "# Algorithm seven - Treynor Black\n",
    "\n",
    "# compute the starting weights alpha / resiudal variance\n",
    "weights = pd.DataFrame()\n",
    "for ticker in tickers:\n",
    "    alpha = alphas.loc[ticker, \"alpha\"]\n",
    "    residual_variance = residual_dev.loc[ticker, \"residual_dev\"]**2\n",
    "    weight = alpha/residual_variance\n",
    "    weights.loc[ticker, \"weight\"] = weight\n",
    "    \n",
    "# scale the weights\n",
    "weights[\"weight\"] = weights[\"weight\"]/weights[\"weight\"].sum()\n",
    "\n",
    "\n",
    "# residual variance of portfolio\n",
    "residual_variance_port = 0\n",
    "alpha_port = 0\n",
    "for ticker in tickers:\n",
    "    residual_variance_port += weights.loc[ticker, \"weight\"]**2 * residual_dev.loc[ticker, \"residual_dev\"]**2\n",
    "    alpha_port += weights.loc[ticker, \"weight\"] * alphas.loc[ticker, \"alpha\"]\n",
    "\n",
    "initial_position = (alpha_port/residual_variance_port) / (expected_return[market]/dev_matrix[market]**2)\n",
    "print(\"Initial Position: \", initial_position)\n",
    "\n",
    "beta_port = 0\n",
    "for ticker in tickers:\n",
    "    beta_port += weights.loc[ticker, \"weight\"]*betas.loc[ticker, \"beta\"]\n",
    "print(\"Beta Portfolio: \", beta_port)\n",
    "\n",
    "adjusted_initial_position = initial_position / (1+ (1-beta_port)*initial_position)\n",
    "print(\"Adjusted Initial Position: \", adjusted_initial_position)\n",
    "\n",
    "m_initial_position = 1-adjusted_initial_position\n",
    "\n",
    "expected_return_port = (m_initial_position+beta_port*adjusted_initial_position)*expected_return[market] + alpha_port*adjusted_initial_position\n",
    "variance_port = (m_initial_position+beta_port*adjusted_initial_position)**2 * dev_matrix[market]**2 + residual_variance_port*adjusted_initial_position**2\n",
    "print(\"Expected Return Portfolio: \", expected_return_port)\n",
    "print(\"Variance Portfolio: \", variance_port)\n",
    "\n",
    "for ticker in weights.index:\n",
    "    final_weights.loc[ticker, \"treynor\"] = adjusted_initial_position*weights.loc[ticker, \"weight\"]\n",
    "\n",
    "# add market to the weights\n",
    "final_weights.loc[market, \"treynor\"] = m_initial_position\n",
    "print(\"Weights: \", final_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE:  2017-10-02\n",
      "                      average_return  total_return   std_dev  sharpe_ratio\n",
      "equal                       0.010723      0.632667  0.030302      0.353877\n",
      "market                      0.010310      0.608268  0.027999      0.368210\n",
      "min_var_mark_return         0.010310      0.608268  0.027413      0.376085\n",
      "equal_var_max_return        0.015176      0.895392  0.028011      0.541797\n",
      "beta_scaled                 0.010207      0.602228  0.030321      0.336636\n",
      "markowitz                   0.011931      0.703932  0.027192      0.438767\n",
      "treynor                     0.008089      0.477272  0.031477      0.256995\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_weights.to_csv(\"data/final_weights.csv\")\n",
    "\n",
    "train_data = None\n",
    "for ticker in tickers+[market]:\n",
    "    \n",
    "    # read in rates\n",
    "    rates = pd.read_csv(\"data/rates/rates_train.csv\")\n",
    "    rates = rates.set_index(\"Date\")\n",
    "    rates = rates[[\"return\"]]\n",
    "    \n",
    "    df = pd.read_csv(\"data/{}/{}_train.csv\".format(ticker, ticker))\n",
    "    df = df.set_index(\"Date\")\n",
    "    df = df[[\"return\"]]\n",
    "    df = df - rates\n",
    "    \n",
    "    df = df.rename(columns={\"return\": ticker})\n",
    "    if train_data is None:\n",
    "        train_data = df\n",
    "    else:  \n",
    "        train_data = pd.merge(train_data, df, on=\"Date\", how=\"inner\")\n",
    "    \n",
    "train_data = train_data.dropna()\n",
    "train_data.to_csv(\"data/test_data.csv\")\n",
    "\n",
    "dates_to_compute_return = [\"2017-10-02\"]\n",
    "# weighted returns\n",
    "weighted_returns = pd.DataFrame()\n",
    "weighted_returns.index = train_data.index\n",
    "for strat in final_weights.columns:\n",
    "    weights = final_weights.loc[:, strat]\n",
    "    temp_returns = np.dot(train_data, weights)\n",
    "    weighted_returns.loc[:, strat] = temp_returns\n",
    "\n",
    "# weighted returns\n",
    "weighted_returns.to_csv(\"data/weighted_returns_train.csv\")\n",
    "\n",
    "for date in dates_to_compute_return:\n",
    "    returns_at_dates = pd.DataFrame(index=final_weights.columns, columns=[\"average_return\", \"total_return\", \"std_dev\", \"sharpe_ratio\"])\n",
    "    weighted_returns_to_now = weighted_returns[weighted_returns.index <= date]\n",
    "    \n",
    "    excess_returns = weighted_returns_to_now\n",
    "    \n",
    "    # compute the average return, total return, and sharpe ratio\n",
    "    average_return = excess_returns.mean()\n",
    "    total_return = excess_returns.sum()\n",
    "    std_dev = excess_returns.std()\n",
    "    sharpe_ratio = average_return/std_dev\n",
    "    \n",
    "    returns_at_dates[\"average_return\"] = average_return\n",
    "    returns_at_dates[\"total_return\"] = total_return\n",
    "    returns_at_dates[\"std_dev\"] = std_dev\n",
    "    returns_at_dates[\"sharpe_ratio\"] = sharpe_ratio\n",
    "    os.makedirs(\"data/FINAL_DATA/\", exist_ok=True)\n",
    "    print(\"DATE: \", date)\n",
    "    print(returns_at_dates)\n",
    "    print(\"\\n\")\n",
    "    returns_at_dates.to_csv(\"data/FINAL_DATA/returns_at_dates_\" + date + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After computation of all strategies we evaluate them below\n",
    "1. Evaluation is done using EXCESS RETURNS to account for varying risk free data\n",
    "2. We evaluate at the five periods described in the announcement\n",
    "3. The first data period does not have all metrics as the deviation cannot be calculated over one period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE:  2017-11-01\n",
      "                      average_return  total_return  std_dev  sharpe_ratio\n",
      "equal                       0.026226      0.026226      NaN           NaN\n",
      "market                      0.030566      0.030566      NaN           NaN\n",
      "min_var_mark_return         0.027111      0.027111      NaN           NaN\n",
      "equal_var_max_return        0.033348      0.033348      NaN           NaN\n",
      "beta_scaled                 0.025444      0.025444      NaN           NaN\n",
      "markowitz                   0.029389      0.029389      NaN           NaN\n",
      "treynor                     0.017906      0.017906      NaN           NaN\n",
      "\n",
      "\n",
      "DATE:  2018-01-01\n",
      "                      average_return  total_return   std_dev  sharpe_ratio\n",
      "equal                       0.036701      0.110104  0.028575      1.284395\n",
      "market                      0.031302      0.093906  0.024697      1.267417\n",
      "min_var_mark_return         0.031582      0.094746  0.025800      1.224098\n",
      "equal_var_max_return        0.036286      0.108859  0.035515      1.021719\n",
      "beta_scaled                 0.035748      0.107244  0.027665      1.292170\n",
      "markowitz                   0.033272      0.099816  0.028951      1.149265\n",
      "treynor                     0.026595      0.079785  0.031850      0.835011\n",
      "\n",
      "\n",
      "DATE:  2018-04-01\n",
      "                      average_return  total_return   std_dev  sharpe_ratio\n",
      "equal                       0.011120      0.066723  0.042492      0.261704\n",
      "market                      0.005237      0.031423  0.035560      0.147278\n",
      "min_var_mark_return         0.006669      0.040013  0.036443      0.182995\n",
      "equal_var_max_return        0.007627      0.045765  0.038647      0.197363\n",
      "beta_scaled                 0.010475      0.062850  0.043029      0.243440\n",
      "markowitz                   0.007063      0.042377  0.035993      0.196230\n",
      "treynor                     0.002656      0.015934  0.048535      0.054716\n",
      "\n",
      "\n",
      "DATE:  2018-10-01\n",
      "                      average_return  total_return   std_dev  sharpe_ratio\n",
      "equal                       0.007442      0.089306  0.035601      0.209045\n",
      "market                      0.004855      0.058262  0.035631      0.136260\n",
      "min_var_mark_return         0.006069      0.072829  0.034107      0.177940\n",
      "equal_var_max_return        0.000949      0.011385  0.042165      0.022501\n",
      "beta_scaled                 0.007285      0.087419  0.035570      0.204805\n",
      "markowitz                   0.004470      0.053644  0.035925      0.124437\n",
      "treynor                     0.008767      0.105201  0.039973      0.219316\n",
      "\n",
      "\n",
      "DATE:  2019-10-01\n",
      "                      average_return  total_return   std_dev  sharpe_ratio\n",
      "equal                       0.011345      0.272276  0.032529      0.348762\n",
      "market                      0.007764      0.186328  0.042018      0.184772\n",
      "min_var_mark_return         0.009044      0.217046  0.037163      0.243347\n",
      "equal_var_max_return        0.008544      0.205049  0.043355      0.197064\n",
      "beta_scaled                 0.010906      0.261746  0.032396      0.336650\n",
      "markowitz                   0.008941      0.214591  0.038576      0.231786\n",
      "treynor                     0.012252      0.294052  0.034365      0.356532\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_weights.to_csv(\"data/final_weights.csv\")\n",
    "\n",
    "test_data = None\n",
    "for ticker in tickers+[market]:\n",
    "    \n",
    "    # read in rates\n",
    "    rates = pd.read_csv(\"data/rates/rates_test.csv\")\n",
    "    rates = rates.set_index(\"Date\")\n",
    "    rates = rates[[\"return\"]]\n",
    "    \n",
    "    df = pd.read_csv(\"data/{}/{}_test.csv\".format(ticker, ticker))\n",
    "    df = df.set_index(\"Date\")\n",
    "    df = df[[\"return\"]]\n",
    "    # df = df - rates\n",
    "    \n",
    "    df = df.rename(columns={\"return\": ticker})\n",
    "    if test_data is None:\n",
    "        test_data = df\n",
    "    else:  \n",
    "        test_data = pd.merge(test_data, df, on=\"Date\", how=\"inner\")\n",
    "    \n",
    "test_data = test_data.dropna()\n",
    "test_data.to_csv(\"data/test_data.csv\")\n",
    "\n",
    "dates_to_compute_return = [\"2017-11-01\", \"2018-01-01\", \"2018-04-01\", \"2018-10-01\", \"2019-10-01\"]\n",
    "# weighted returns\n",
    "weighted_returns = pd.DataFrame()\n",
    "weighted_returns.index = test_data.index\n",
    "for strat in final_weights.columns:\n",
    "    weights = final_weights.loc[:, strat]\n",
    "    temp_returns = np.dot(test_data, weights)\n",
    "    weighted_returns.loc[:, strat] = temp_returns\n",
    "\n",
    "# weighted returns\n",
    "weighted_returns.to_csv(\"data/weighted_returns.csv\")\n",
    "\n",
    "for date in dates_to_compute_return:\n",
    "    returns_at_dates = pd.DataFrame(index=final_weights.columns, columns=[\"average_return\", \"total_return\", \"std_dev\", \"sharpe_ratio\"])\n",
    "    weighted_returns_to_now = weighted_returns[weighted_returns.index <= date]\n",
    "    \n",
    "    excess_returns = weighted_returns_to_now\n",
    "    \n",
    "    # compute the average return, total return, and sharpe ratio\n",
    "    average_return = excess_returns.mean()\n",
    "    total_return = excess_returns.sum()\n",
    "    std_dev = excess_returns.std()\n",
    "    sharpe_ratio = average_return/std_dev\n",
    "    \n",
    "    returns_at_dates[\"average_return\"] = average_return\n",
    "    returns_at_dates[\"total_return\"] = total_return\n",
    "    returns_at_dates[\"std_dev\"] = std_dev\n",
    "    returns_at_dates[\"sharpe_ratio\"] = sharpe_ratio\n",
    "    os.makedirs(\"data/FINAL_DATA/\", exist_ok=True)\n",
    "    print(\"DATE: \", date)\n",
    "    print(returns_at_dates)\n",
    "    print(\"\\n\")\n",
    "    returns_at_dates.to_csv(\"data/FINAL_DATA/returns_at_dates_\" + date + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
